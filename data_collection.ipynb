{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import chardet\n",
    "\n",
    "news_companies = {\n",
    "    \"Inquirer.net\": \"https://www.inquirer.net/\",\n",
    "    \"Manila Bulletin\": \"https://mb.com.ph/\",\n",
    "    \"The Asian Journal USA\": \"https://asianjournal.com/\",\n",
    "    \"The Manila Times\": \"https://www.manilatimes.net/\",\n",
    "    \"Business World\": \"https://www.bworldonline.com/\",\n",
    "    \"Eagle News\": \"https://www.eaglenews.ph/\",\n",
    "    \"Metro Cebu News\": \"https://metrocebu.news/\",\n",
    "    \"Tempo\": \"https://tempo.com.ph/\",\n",
    "    \"Abante Tonite\": \"https://tonite.abante.com.ph/\",\n",
    "    \"Philippine News Agency\": \"https://www.pna.gov.ph/\",\n",
    "    \"InterAksyon\": \"https://interaksyon.philstar.com/\",\n",
    "    \"Business Mirror\": \"https://businessmirror.com.ph/\",\n",
    "    \"The Summit Express\": \"https://www.thesummitexpress.com/\",\n",
    "    \"Our Daily News Online\": \"https://ourdailynewsonline.com/\",\n",
    "    \"Current PH\": \"https://currentph.com/\",\n",
    "    \"SunStar Philippines\": \"https://www.sunstar.com.ph/\",\n",
    "    \"Rappler\": \"https://www.rappler.com/\",\n",
    "    \"The Bohol Chronicle\": \"https://www.boholchronicle.com.ph/\",\n",
    "    \"Baguio Midland Courier\": \"https://www.baguiomidlandcourier.com.ph/\",\n",
    "    \"GMA News Online\": \"https://www.gmanetwork.com/news/\",\n",
    "    \"Cebu Daily News\": \"https://cebudailynews.inquirer.net/\",\n",
    "    \"ABS-CBN News\": \"https://news.abs-cbn.com/\",\n",
    "    \"Philstar.com\": \"https://www.philstar.com/\",\n",
    "    \"Manila Standard\": \"https://manilastandard.net/\",\n",
    "    \"Daily Tribune\": \"https://tribune.net.ph/\",\n",
    "    \"Davao Today\": \"https://davaotoday.com/\",\n",
    "    \"Sunday Punch\": \"https://punch.dagupan.com/\",\n",
    "    \"Visayan Daily Star\": \"https://visayandailystar.com/\",\n",
    "    \"PTV News\": \"https://ptvnews.ph/\",\n",
    "    \"Mindanao Times\": \"https://mindanaotimes.com.ph/\",\n",
    "    \"PhilNews.XYZ\": \"https://philnews.xyz/\",\n",
    "    \"Northern Dispatch\": \"https://nordis.net/\"\n",
    "}\n",
    "\n",
    "news_topics = [\n",
    "    \"inflation\", \n",
    "    \"economy\", \n",
    "    \"business\", \n",
    "    \"technology\", \n",
    "    \"health\", \n",
    "    \"environment\", \n",
    "    \"welfare\", \n",
    "    \"politics\", \n",
    "    \"foreign_affairs\"\n",
    "]\n",
    "\n",
    "raw_folder = {\n",
    "  'business' : ['Business1.csv', 'Business2.csv', 'Business3.csv', 'Business5.csv', 'Business6.csv', 'Business7.csv', 'Business8.csv', ],\n",
    "  'economy' : ['Economy1.csv','Economy2.csv'],\n",
    "  'environment' : ['Environment1.csv','Environment2.csv','Environment3.csv','Environment4.csv',],\n",
    "  'foreign_affairs' : ['ForeignAffairs1.csv'],\n",
    "  'health': ['Health1.csv','Health2.csv','Health3.csv','Health4.csv',],\n",
    "  'inflation':['Inflation1.csv'],\n",
    "  'politics':['Politics1.csv','Politics2.csv'],\n",
    "  'technology':['Technology1.csv','Technology2.csv','Technology3.csv','Technology4.csv','Technology5.csv',],\n",
    "  'welfare':['Welfare1.csv']\n",
    "}\n",
    "\n",
    "filtered_folder = {\n",
    "  'business':'business_tally.csv',\n",
    "  'economy':'economy_tally.csv',\n",
    "  'environment' :'environment_tally.csv',\n",
    "  'foreign_affairs' :'foreign_affairs_tally.csv',\n",
    "  'health':'health_tally.csv',\n",
    "  'inflation':'inflation_tally.csv',\n",
    "  'politics':'politics_tally.csv',\n",
    "  'technology':'technology_tally.csv',\n",
    "  'welfare':'welfare_tally.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an empty dataset with columns month, day and all the news sources\n",
    "def empty_dataset(topic):\n",
    "    # Define the header columns\n",
    "    header_columns = [\"Month\", \"Day\"] + list(news_companies.keys())\n",
    "\n",
    "    # Specify the folder and filename\n",
    "    folder_name = \"filtered_datasets\"\n",
    "    filename = os.path.join(folder_name, topic + \"_tally.csv\")\n",
    "\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Generate the date range from January 1 to April 30\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(2024, 4, 30)\n",
    "    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    # Prepare the rows with preset values\n",
    "    rows = []\n",
    "    for date in date_range:\n",
    "        month = date.strftime(\"%B\")  # Full month name\n",
    "        day = date.day\n",
    "        row = [month, day] + [0] * len(news_companies)\n",
    "        rows.append(row)\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        # Create a csv writer object\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        \n",
    "        # Write the header row\n",
    "        csvwriter.writerow(header_columns)\n",
    "        \n",
    "        # Write the rows with preset values\n",
    "        csvwriter.writerows(rows)\n",
    "    print(f\"Finished creating '{topic}' dataset as '{topic}_tally.csv'.\")\n",
    "\n",
    "# Creates a dataset per topic included in news_topics\n",
    "def create_empty_datasets():\n",
    "    for topic in news_topics:\n",
    "        print(f\"Creating final dataset for {topic}.\")\n",
    "        empty_dataset(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that increments the cell specified by the dataset, month, day and source\n",
    "def record(filename, month, day, header):\n",
    "    df = pd.read_csv(filename)\n",
    "    # Find the row that matches the month and day\n",
    "    row_index = df[(df['Month'] == month) & (df['Day'] == day)].index\n",
    "\n",
    "    if not row_index.empty:\n",
    "        # Increment the value in the specified column\n",
    "        df.at[row_index[0], header] += 1\n",
    "    else:\n",
    "        print(f\"No matching row found for {month} {day}\")\n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"'{filename}''{day}''{month}''{header}' recorded\")\n",
    "\n",
    "# Sample call\n",
    "# record(\"health_tally.csv\",\"January\", 1, \"Inquirer.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Inquirer.net\n"
     ]
    }
   ],
   "source": [
    "def is_url_from_list(url):\n",
    "    # Ensure url is a string to avoid TypeError\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    for key, value in news_companies.items():\n",
    "        if value in url:\n",
    "            return key\n",
    "    return False\n",
    "\n",
    "# Sample calls\n",
    "url = \"https://www.inquiresr.net/some-article/d\"\n",
    "result = is_url_from_list(url)\n",
    "print(result)  # Output: Inquirer.net\n",
    "\n",
    "\n",
    "url = \"https://www.inquirer.net/some-article/d\"\n",
    "result = is_url_from_list(url)\n",
    "print(result)  # Output: Flase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the date and extract month and day\n",
    "def parse_date(row):\n",
    "    if isinstance(row, str):  # Check if the input is a string\n",
    "        try:\n",
    "            date_str = row.split('\\t')[0]\n",
    "            date_time_str = date_str.strip()\n",
    "            date_obj = datetime.strptime(date_time_str, '%d-%b-%Y %I:%M%p')\n",
    "            return date_obj\n",
    "        except ValueError:\n",
    "            return None  # Return None for invalid dates\n",
    "    else:\n",
    "        return None  # Return None for non-string inputs\n",
    "\n",
    "# Function to filter each row in each dataset provided the filename of the raw dataset, and the filename of the topic dataset\n",
    "def raw_data_processing(filename, topicDataset):\n",
    "    print(f\"Filtering raw dataset: '{filename}'.\")\n",
    "    # Dictionary to map month numbers to month names\n",
    "    month_dict = {\n",
    "        1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
    "        5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n",
    "        9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "    }\n",
    "\n",
    "    input_filename = os.path.join(\"raw_datasets\", filename)\n",
    "    output_filename = os.path.join(\"filtered_datasets\", topicDataset)\n",
    "\n",
    "    # Detect the encoding of the CSV file\n",
    "    with open(input_filename, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    # Read the CSV file with error handling and correct delimiter\n",
    "    df = pd.read_csv(input_filename, encoding=encoding, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        date_str = row['Date']\n",
    "        url = row['URL']\n",
    "        \n",
    "        # Parse the date to get month and day\n",
    "        date_obj = parse_date(date_str)\n",
    "        if date_obj:\n",
    "            month = month_dict[date_obj.month]\n",
    "            day = date_obj.day\n",
    "            # Print or store the values as needed\n",
    "            header = is_url_from_list(url)\n",
    "            if header != False:\n",
    "                record(output_filename, month, day, header)\n",
    "        else:\n",
    "            print(f\"Invalid date format in row {index}\")\n",
    "    print(f\"Finished filtering raw dataset: '{filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset inflation\n",
      "CSV file 'inflation_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset economy\n",
      "CSV file 'economy_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset business\n",
      "CSV file 'business_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset technology\n",
      "CSV file 'technology_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset health\n",
      "CSV file 'health_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset environment\n",
      "CSV file 'environment_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset welfare\n",
      "CSV file 'welfare_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset politics\n",
      "CSV file 'politics_tally.csv' created in folder 'filtered_datasets'.\n",
      "Creating dataset foreign_affairs\n",
      "CSV file 'foreign_affairs_tally.csv' created in folder 'filtered_datasets'.\n"
     ]
    }
   ],
   "source": [
    "# Main code block for execution\n",
    "create_empty_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "  # Loop through each topic\n",
    "  for topic in news_topics:\n",
    "      # Loop through each file in the current topic's folder\n",
    "      for filename in raw_folder[topic]:\n",
    "          # Construct the output filename using the filtered_folder dictionary\n",
    "          output_filename = filtered_folder[topic]\n",
    "          # Call the processing function with the constructed filenames\n",
    "          raw_data_processing(filename, output_filename)\n",
    "  print(f\"Data collection completed.\")\n",
    "\n",
    "collect_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
